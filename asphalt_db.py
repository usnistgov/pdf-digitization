# -*- coding: utf-8 -*-
"""EPD_Paper_Asphalt_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19EqZmyTB5wmnI0HnLYL2Eg2cT_T0Jy-V
"""

# from google.colab import drive
# drive.mount('/content/drive')

import csv
import os
import numpy as np
import pandas as pd
from bs4 import BeautifulSoup
import re
import datetime

def split_address(address):
    # Split the address into individual parts
    parts = address.split(',')

    # Get the street, city, state, and zip
    street = parts[0].strip()
    city = parts[1].strip()
    state_zip = parts[2].strip().split(' ')
    state = state_zip[0] or 'NA'
    if len(state_zip) > 1:
      zipcode = state_zip[1] or 'NA'
    else:
      zipcode = state_zip

    return street, city, state, zipcode

def dateConvert(date):
    # Python’s strptime does not recognize "Sept" as a valid abbreviation for September — only "Sep"
    date = date.replace("Sept", "Sep")

    # Remove ordinal suffixes like 'st', 'nd', 'rd', 'th'
    date = re.sub(r'(\d+)(st|nd|rd|th)', r'\1', date)

    date_formats = [
        "%Y-%m-%d",       # 2023-06-06
        "%B %d, %Y",      # June 6, 2023
        "%m/%d/%Y",       # 06/06/2023
        "%m-%d-%Y",       # 06-06-2023
        "%m/%d/%y",       # 6/6/23
        "%b. %d, %Y",     # Sept. 26, 2023
        "%b %d, %Y",      # Sept 22, 2023
        "%b.%d, %Y"       # Sept.22, 2023
    ]

    for fmt in date_formats:
      try:
          date_obj = datetime.datetime.strptime(date, fmt)
          return date_obj.strftime("%m/%d/%Y")
      except ValueError:
          continue

    # If none of the formats matched, return the original string
    return date

def removeParentheses(text):
    start_index = text.find("(")
    if start_index != -1:
      return text[:start_index].strip()
    else:
        return text

def safe_find_and_clean(soup, tag_names, attrs, use_remove_paren=True):
  elem = soup.find(tag_names, attrs)
  if elem and elem.text:
    text = elem.text.strip()
    return removeParentheses(text) if use_remove_paren else text
  else:
    return 'NA'

def process_ids(soup, prefix, items, exclude_items, use_remove_paren=True):
  results = []
  for item in items:
    if item not in exclude_items:
      id_str = f"{prefix}{item}_1"
      results.append(safe_find_and_clean(soup, ['span'], {'id': id_str}, use_remove_paren))
  return results

# Constant

agg_size_mm_reported = ''
agg_size_mm = ''

# Note: The following values are same for all the EPDs at the time of publishing. Hence, they have been hardcoded for simplicity (and speed). If they change, the code will need modifications.
pcr_review_individual = "Joep Meijer"
pcr_review_company = "TheRightenvironment"
lca_3pv_individual = "Trisha Montalbo"
lca_3pv_company = "John Beath Environmental LLC"
epd_3pv_individual = "Trisha Montalbo"
epd_3pv_company = "John Beath Environmental LLC"

# tables
table1Cols = []
for i in range(11):
    table1Cols.append(f"Component {i+1}")
    table1Cols.append(f"Material {i+1}")
    table1Cols.append(f"Weight % {i+1}")

table2Cols = []
for i in range(10):
    table2Cols.append(f"Chemical Name {i+1}")
    table2Cols.append(f"CAS No {i+1}")
    table2Cols.append(f"Weight % {i+1}")

gapCols = []
for i in range(0,11):
  gapCols.append(f"Gap {i+1}")

table4Cols = ["GWP-100 (unit)",	"GWP-100 A1",	"GWP-100 A2",	"GWP-100 A3",	"GWP-100 A1-A3","ODP (unit)","OPD A1","OPD A2","OPD A3","OPD A1-A3","EP (unit)", "EP A1",	"EP A2",	"EP A3",	"EP A1-A3",
              "AP (unit)",	"AP A1",	"AP A2",	"AP A3",	"AP A1-A3","POCP (unit)",	"POCP A1",	"POCP A2",	"POCP A3",	"POCP A1-A3"]

table5Cols = ["RPRe (unit)",	"RPRe A1",	"RPRe A2",	"RPRe A3",	"RPRe A1-A3","RPRm (unit)",	"RPRm A1",	"RPRm A2",	"RPRm A3",	"RPRm A1-A3","NRPRe (unit)",	"NRPRe A1",	"NRPRe A2",	"NRPRe A3",	"NRPRe A1-A3",
              "NRPRm (unit)",	"NRPRm A1",	"NRPRm A2",	"NRPRm A3",	"NRPRm A1-A3","SM (unit)",	"SM A1",	"SM A2",	"SM A3",	"SM A1-A3","RSF (unit)",	"RSF A1",	"RSF A2",	"RSF A3",	"RSF A1-A3",
              "NRSF (unit)",	"NRSF A1",	"NRSF A2",	"NRSF A3",	"NRSF A1-A3", "RE (unit)",	"RE A1",	"RE A2",	"RE A3",	"RE A1-A3", "FW (unit)",	"FW A1",	"FW A2",	"FW A3",	"FW A1-A3",
              "ADP fossil (unit)",	"ADP fossil A1",	"ADP fossil A2",	"ADP fossil A3",	"ADP fossil A1-A3"]

table6Cols = ["HWD (unit)",	"HWD A1",	"HWD A2",	"HWD A3",	"HWD A1-A3","NHWD (unit)",	"NHWD A1",	"NHWD A2",	"NHWD A3",	"NHWD A1-A3","RWD-HL (unit)",	"RWD-HL A1",	"RWD-HL A2",	"RWD-HL A3",	"RWD-HL A1-A3",
              "RWD-LL (unit)",	"RWD-LL A1",	"RWD-LL A2",	"RWD-LL A3",	"RWD-LL A1-A3","CRU (unit)",	"CRU A1",	"CRU A2",	"CRU A3",	"CRU A1-A3","MFR (unit)","MFR A1",	"MFR A2",	"MFR A3",	"MFR A1-A3",
              "MFER (unit)","MFER A1",	"MFER A2",	"MFER A3",	"MFER A1-A3","REE (unit)",	"REE A1",	"REE A2",	"REE A3",	"REE A1-A3"	]

table7Cols = ["GHG luc (unit)",	"GHG luc A1",	"GHG luc A2",	"GHG luc A3",	"GHG luc A1-A3","BCPR (unit)",	"BCPR A1",	"BCPR A2",	"BCPR A3",	"BCPR A1-A3","BCPE (unit)",	"BCPE A1",	"BCPE A2",	"BCPE A3",	"BCPE A1-A3",
              "BCWR (unit)",	"BCWR A1",	"BCWR A2",	"BCWR A3",	"BCWR A1-A3","BCWN (unit)",	"BCWN A1",	"BCWN A2",	"BCWN A3",	"BCWN A1-A3","CCAL (unit)",	"CCAL A1",	"CCAL A2",	"CCAL A3",	"CCAL A1-A3",
              "CCAR (unit)",	"CCAR A1",	"CCAR A2",	"CCAR A3",	"CCAR A1-A3"]

table8Cols = ["GHG re market based accounting (unit)","GHG re market based accounting A1"	,"GHG re market based accounting A2","GHG re market based accounting A3","GHG re market based accounting A1-A3",]

table9Cols = ["BC bio (unit)","BC bio A1","BC bio A2","BC bio A3","BC bio A1-A3"]

CSVcolumns = ["Sl No",	"Declaration owner",	"Plant name",	"Plant Type",	"Address (full)","Address (Street)","Address (City)","Address(State)", "Address(Zip)",	"Program operator", "LCA and EPD tool developer",
              "PCR",	"PCR version","PCR review (individual)",	"PCR review (Company)",	"LCA 3PV (individual)",	"LCA 3PV (Company)",	"EPD 3PV (individual)",	"EPD 3PV (Company)",	"EPD link",
              "Declaration number", "UNSPS Code",	"Reference ISO - 1",	"Reference ISO - 2",	"Declared product","Specification Entity",	"Specification",	"Gradation Type","Mix design methods",
              "Nominal Max. Aggregate size as reported", "Nominal Max. Aggregate size in mm",	"Performance Grade of asphalt binder",	"Customer number", "Asphalt Mix category",	"Asphalt production temperature range (lower)",
              "Asphalt production temperature range (upper)", "Data completeness statement  (Y/N)",	"Individual raw material level cut off %",	"Product level cut off %",	"Software version",
              "Date of Issue", "period of validity",	"Declared Unit Qty","Declared Unit",	"LCI collection location",	"LCI collection duration",	"LCI collection Start date",	"Allocation",
              *table1Cols, *gapCols, *table2Cols, "Impact assessment method", "LCIA version",*table4Cols,*table5Cols,*table6Cols,*table7Cols,*table8Cols,*table9Cols
            ]

# Define the user directory input
html_dir  = "/content/drive/MyDrive/asphalt-htmls/EPDs as of 1 14 2025/state-wise/"
html_files = [f for f in os.listdir(html_dir) if f.endswith('.htm')]

data = []
index=0

for root, dirs, files in os.walk(html_dir):
  for file in files:
    if file.endswith('.htm'):
      html_file = os.path.join(root, file)
      with open(html_file, 'r') as f:
        html_content = f.read()
        soup = BeautifulSoup(html_content, 'html.parser')

        # Extract data based on class names
        slNo = index+1
        declaration_owner = soup.find('span', {'id': 't2_1'}).text.replace("is an asphalt mixture producer.", "").strip()
        plant_name = soup.find('span', {'id': 't4_1'}).text.strip().split(',')[0].strip() or "NA"
        plant_type = re.findall('(stationary|portable)',soup.find('span', {'id': 't4_1'}).text.strip())[0] or "NA"

        address = soup.find('div', {'id': 't8_1'}).text.strip() or "NA"
        [street,city,state,zip] = split_address(address)

        operator = soup.find('span', {'id': 'p12_t1l_1'}).text.strip() or "NA"
        tool_dev = soup.find('span', {'id': 'p12_t18_1'}).text.strip() or "NA"
        pcr = soup.find('span', {'id': 'p12_t1k_1'}).text.split(',')[0].strip() or "NA"
        pcr_version = re.findall('\d.*',soup.find('span', {'id': 'p12_t1k_1'}).text.strip())[0] or "NA"

        epd_link = soup.find('span', {'id': 't1g_1'}).text.split('at ')[1] or "NA"
        declaration_num = soup.find('span', {'id': 't16_1'}).text.strip() or "NA"

        unspsc = re.findall('UNSPSC Code.\d*', soup.find(string=lambda text: 'UNSPSC Code' in text))[0] or "NA"
        unspsc_code = re.findall(r'\d+', unspsc)[0]

        iso1 = re.findall('ISO.\d*\:\d*', soup.find('span', {'id': 'tv_1'}).text.strip())[0] or "NA"
        iso2 = re.findall('ISO.\d*\:\d*',soup.find('span', {'id': 'tx_1'}).text.strip())[0] or "NA"
        declared_prod =  re.findall(':([^;]*)',soup.find('span', {'id': 'te_1'}).text.strip())[0] or "NA"
        specification_entity =  re.findall(':([^;]*)',soup.find('span', {'id': 'tf_1'}).text.strip())[0] or "NA"
        specification =  re.findall(':([^;]*)',soup.find('span', {'id': 'tg_1'}).text.strip())[0] or "NA"
        gradation_type =  re.findall(':([^;]*)',soup.find('span', {'id': 'th_1'}).text.strip())[0] or "NA"
        design_methods =  re.findall(':([^;]*)',soup.find('span', {'id': 'ti_1'}).text.strip())[0] or "NA"

        agg_size_reported = re.findall(':([^;]*)',soup.find('span', {'id': 'tj_1'}).text.strip())[0]
        if "Not Reported" in agg_size_reported:
          continue
        elif "inches" in agg_size_reported:
          agg_size_inches = re.findall(r'\d+(?:\.\d+)?', agg_size_reported)[0] or "NA"
          agg_size_mm = float(agg_size_inches) * 25.4
        else:
          value = re.findall(r'\d+(?:\.\d+)?', agg_size_reported)
          agg_size_mm_reported = value[0]
          if type(value) == list:
            agg_size_mm = value[0] or "NA"

        perf_grade =  re.findall(':([^;]*)', soup.find('span', {'id': 'tk_1'}).text.strip())[0] or "NA"
        cust_num = re.findall(':([^;]*)',soup.find('span', {'id': 'tl_1'}).text.strip())[0] or "NA"

        mix_categoryText = re.findall('(Hot Mix Asphalt \(HMA\))|(Warm Mix Asphalt \(WMA\))|(Cold Mix Asphalt \(WMA\))',soup.find('span', {'id': 'tm_1'}).text.strip())
        mix_category = mix_categoryText[0][0] if mix_categoryText else "NA"
        if len(mix_categoryText[0][0]) > 1:
          mix_category = mix_categoryText[0][0] if mix_categoryText else "NA"
        else:
            mix_category = mix_categoryText[0][1] if mix_categoryText else "NA"

        temp = re.findall(r'(\d+(?:\.\d+)?)\s*to\s*(\d+(?:\.\d+)?)\s*[°C]',soup.find('span',{'id':'tm_1'}).text.strip())[0]
        lower_temp = temp[0]
        higher_temp = temp[1]

        data_completeness = soup.find('div',{'id':'tr_1'}) != None
        if data_completeness == True:
          level = re.findall('(\d+)%',soup.find('div',{'id':'tr_1'}).text.strip())
          raw_mat_level = level[0]+"%"
          prod_level = level[1]+"%"
        else:
          raw_mat_level = 'NA'
          prod_level = 'NA'

        soft_version = soup.find('span', {'id': 't18_1'}).text.strip()  or "NA"
        date = soup.find('span', {'id': 't1a_1'}).text.strip() or "NA"
        date_of_issue = dateConvert(date)
        period_validity = dateConvert(soup.find('span', {'id':'t1c_1'}).text.strip()) or "NA"
        declared_unit_full = re.findall('(\d+ metric tonne)',soup.find('span',attrs={'id':'p3_to_1'}).text.strip())[0].split()
        declared_unit_num = declared_unit_full[0] or "NA"
        declared_unit = " ".join(declared_unit_full[1:]) or "NA"
        collection_loc_text = re.findall(r'\b(\w+)\s*[\-‑]\s*(\w+)\b',soup.find('span',attrs={'id':'p3_t1c_1'}).text.strip())[0]
        collection_loc = collection_loc_text[0]+"-"+collection_loc_text[1]
        collection_dur = soup.find('span', {'id': 't1e_1'}).text.split('from a ')[1].split(' period')[0]
        collection_start_date = re.search(r'beginning on ([A-Z][a-z]+[.,]?\s+\d{1,2},\s+\d{4})[.]', soup.find('span', {'id': 't1e_1'}).text).group(1)
        collection_start = dateConvert(collection_start_date)
        mass_basisText = re.findall('(\w+)\s+(\w+)\s*\.',soup.find('span',{'id':'p3_t1l_1'}).text.strip())[0]
        mass_basis = mass_basisText[0] + ' ' + mass_basisText[1]

        impactText = re.findall('(\w+\s+v\d+\.\d+)',soup.find('span',{'id':'p4_t1b_1'}).text.strip())[0].split()
        impact_method = impactText[0]
        lcia_version = impactText[1]

        # ------table1------
        gap = []
        table1 = []
        table1Rows = 33 # 11 rows * 3 columns
        for row in soup.find_all('tr', class_='ingredient-row'):
          row_data = [td.text.strip() or "NA" for td in row.find_all('td')]
          table1.extend(row_data)
          if('*' in row_data[1]):
            gap.append(row_data[1])
        while len(gap) < 11:
          gap.append("NA")
        remainingRows = table1Rows - len(table1)
        if remainingRows > 0:
          table1.extend(['NA'] * remainingRows)
        # ------table1------

        # ------table2------
        table2 = []
        table2Rows = 30 # 10 rows * 3 columns
        for row in soup.find_all('tr', class_='sds-row'):
          row_data = [td.text.strip() or "NA" for td in row.find_all('td')]
          table2.extend(row_data)
        if len(table2) < table2Rows:
          table2.extend(["NA"] * (table2Rows - len(table2)))
        # ------table2------

        # ------table4------
        table4 = []
        for i in range(ord('n'), ord('r') + 1):
          text = soup.find(['span', 'div'], {'id': f'p5_t{chr(i)}_1'}).text.strip()
          table4.append(removeParentheses(text))

        coldn = soup.find(['span', 'div'], {'id': f'p5_tw_1'}).text.strip() + " " + soup.find(['span', 'div'], {'id': f'p5_tx_1'}).text.strip()
        coldo = soup.find(['span', 'div'], {'id': f'p5_ty_1'}).text.strip()
        coldp = soup.find(['span', 'div'], {'id': f'p5_tz_1'}).text.strip()
        coldq = soup.find(['span', 'div'], {'id': f'p5_t10_1'}).text.strip()
        coldr = soup.find(['span', 'div'], {'id': f'p5_t11_1'}).text.strip()

        table4.append(removeParentheses(coldn))
        table4.append(removeParentheses(coldo))
        table4.append(removeParentheses(coldp))
        table4.append(removeParentheses(coldq))
        table4.append(removeParentheses(coldr))

        for i in range(15, 19 + 1):
          text = soup.find(['span', 'div'], {'id': f'p5_t{i}_1'}).text.strip()
          table4.append(removeParentheses(text))

        for i in range(ord('d'), ord('h') + 1):
          text = soup.find(['span', 'div'], {'id': f'p5_t1{chr(i)}_1'}).text.strip()
          table4.append(removeParentheses(text))

        for i in range(ord('m'), ord('q') + 1):
          text = soup.find(['span', 'div'], {'id': f'p5_t1{chr(i)}_1'}).text.strip()
          table4.append(removeParentheses(text))
        # ------table4------

        # ------table5------
        table5 = []
        chars_1 = [chr(i) for i in range(ord('n'), ord('v') + 1)]
        exclude_1 = ['p', 'r', 't', 'v']
        table5.extend(process_ids(soup, 'p6_t', chars_1, exclude_1))

        digits_1 = list(range(0, 10))
        exclude_2 = [2, 4, 6, 8, 9]
        table5.extend(process_ids(soup, 'p6_t1', digits_1, exclude_2))

        chars_2 = [chr(i) for i in range(ord('d'), ord('z') + 1)]
        exclude_3 = ["h", "f", "j", "l", "m", "n", "o", "p", "s", "u", "w", "y", "z"]
        table5.extend(process_ids(soup, 'p6_t1', chars_2, exclude_3))

        digits_2 = list(range(1, 10))
        exclude_4 = [3, 5, 7, 9]
        table5.extend(process_ids(soup, 'p6_t2', digits_2, exclude_4))

        chars_3 = [chr(i) for i in range(ord('c'), ord('z') + 1)]
        exclude_5 = ['e', 'g', 'i', 'k', 'l', "m", 'p', 'r', 't', 'v', 'w', 'x']
        for ch in chars_3:
          if ch not in exclude_5:
            id_str = f'p6_t2{ch}_1'
            elem = soup.find(['span'], {'id': id_str})
            text = elem.text.strip() if elem and elem.text else 'NA'
            table5.append(text)

        digits_3 = list(range(1, 10))
        exclude_6 = [2, 4, 6, 7, 8, 9]
        table5.extend(process_ids(soup, 'p6_t3', digits_3, exclude_6))

        table5.append('m\u00b3')

        chars_4 = [chr(i) for i in range(ord('b'), ord('v') + 1)]
        exclude_7 = ['c', 'e', 'g', 'i', 'j', 'k', 'l', "m", 'p', 'r', 't', 'v']
        table5.extend(process_ids(soup, 'p6_t3', chars_4, exclude_7))
        # ------table5------

        # ------table6------
        table6 = []
        chars_6_1 = [chr(i) for i in range(ord('l'), ord('z') + 1)]
        exclude_6_1 = ['p', 'r', 's', 't', 'u', 'z']
        table6.extend(process_ids(soup, 'p7_t', chars_6_1, exclude_6_1, use_remove_paren=False))

        digits_6_1 = list(range(0, 10))
        exclude_6_2 = [1, 2, 3, 4, 6]
        for i in digits_6_1:
          if i not in exclude_6_2:
            id_str = f'p7_t1{i}_1'
            text = safe_find_and_clean(soup, ['span'], {'id': id_str}, True)
            if i == 5:
                table6.append('kg or m\u00b3')
            else:
                table6.append(text)

        table6.append(safe_find_and_clean(soup, ['span'], {'id': 'p7_t1b_1'}, True))
        table6.append('kg or m\u00b3')

        chars_6_2 = [chr(i) for i in range(ord('j'), ord('z') + 1)]
        exclude_6_3 = ["m", "o", "p", "q", "r", "w", "y", 'z']
        table6.extend(process_ids(soup, 'p7_t1', chars_6_2, exclude_6_3))

        digits_6_3 = list(range(1, 7))
        exclude_6_4 = [5]
        table6.extend(process_ids(soup, 'p7_t2', digits_6_3, exclude_6_4))

        chars_6_3 = [chr(i) for i in range(ord('b'), ord('s') + 1)]
        exclude_6_5 = ['f', 'h', 'i', 'j', 'k', 'l', 'q', 's']
        table6.extend(process_ids(soup, 'p7_t2', chars_6_3, exclude_6_5))
        # ------table6------

        # ------table7------
        table7 = ['kg CO2 eq']
        chars_7_1 = [chr(i) for i in range(ord('u'), ord('x') + 1)]
        table7.extend(process_ids(soup, 'p8_t', chars_7_1, [], use_remove_paren=True))

        table7.append('kg CO2')

        digits_7_1 = list(range(5, 10))
        exclude_7_1 = [6]
        for i in digits_7_1:
          if i != 6:
            id_str = f'p8_t1{i}_1'
            text = safe_find_and_clean(soup, ['span','div'], {'id': id_str})
            table7.append(text)

        table7.append('kg CO2')

        chars_7_2 = [chr(i) for i in range(ord('i'), ord('z') + 1)]
        exclude_7_2 = ["m", 'n', "o", "p", "q", 's', 'u', "w", "y"]
        for ch in chars_7_2:
          if ch not in exclude_7_2:
            id_str = f'p8_t1{ch}_1'
            if ch == 'r':
                table7.append('kg CO2')
            else:
                table7.append(safe_find_and_clean(soup, ['span','div'], {'id': id_str}))

        table7.append('kg CO2')
        table7.append(safe_find_and_clean(soup, ['span'], {'id': 'p8_t27_1'}))
        table7.append(safe_find_and_clean(soup, ['span'], {'id': 'p8_t29_1'}))

        chars_7_3 = [chr(i) for i in range(ord('a'), ord('v') + 1)]
        exclude_7_3 = ['b', 'd', 'e', 'f', 'h', 'j', 'n', 'o', 'p', 'r']
        for ch in chars_7_3:
          if ch not in exclude_7_3:
            id_str = f'p8_t2{ch}_1'
            if ch == 'g' or ch == 'q':
              table7.append('kg CO2')
            else:
              table7.append(safe_find_and_clean(soup, ['span','div'], {'id': id_str}))
        # ------table7------

        # ------table8------
        coljc = soup.find(['span'], {'id': f'p9_t18_1'}).text.strip() + soup.find(['span'], {'id': f'p9_t19_1'}).text.strip() + soup.find(['span'], {'id': f'p9_t1a_1'}).text.strip()
        coljd=soup.find(['span'], {'id': f'p9_t1b_1'}).text.strip()
        colje=soup.find(['span'], {'id': f'p9_t1c_1'}).text.strip()
        coljf=soup.find(['span'], {'id': f'p9_t1d_1'}).text.strip()
        coljg=soup.find(['span'], {'id': f'p9_t1f_1'}).text.strip()

        table8 = [removeParentheses(coljc),removeParentheses(coljd),removeParentheses(colje),removeParentheses(coljf),removeParentheses(coljg)]
        # ------table8------

        # ------table9------
        coljh = soup.find(['span'], {'id': f'p9_t2b_1'}).text.strip() + soup.find(['span'], {'id': f'p9_t2c_1'}).text.strip() + soup.find(['span'], {'id': f'p9_t2d_1'}).text.strip()
        colji=soup.find(['span'], {'id': f'p9_t2e_1'}).text.strip()
        coljj=soup.find(['span'], {'id': f'p9_t2f_1'}).text.strip()
        coljk=soup.find(['span'], {'id': f'p9_t2g_1'}).text.strip()
        coljl=soup.find(['span'], {'id': f'p9_t2i_1'}).text.strip()

        table9 = [removeParentheses(coljh),removeParentheses(colji),removeParentheses(coljj),removeParentheses(coljk),removeParentheses(coljl)]
        # ------table9------

        index+=1
        data.append([slNo, declaration_owner, plant_name, plant_type, address, street, city, state, zip, operator, tool_dev, pcr, pcr_version, pcr_review_individual,pcr_review_company,
                    lca_3pv_individual,lca_3pv_company,epd_3pv_individual,epd_3pv_company,epd_link, declaration_num, unspsc_code, iso1, iso2, declared_prod, specification_entity, specification,
                    gradation_type, design_methods, agg_size_reported,  agg_size_mm, perf_grade, cust_num, mix_category, lower_temp, higher_temp, data_completeness, raw_mat_level, prod_level,
                    soft_version, date_of_issue, period_validity, declared_unit_num, declared_unit, collection_loc, collection_dur,collection_start, mass_basis, *table1, *gap, *table2,
                    impact_method, lcia_version, *table4, *table5, *table6, *table7, *table8, *table9])

# Create a DataFrame from the extracted data
df = pd.DataFrame(data,columns=CSVcolumns)

# Save the DataFrame as a CSV file
df.to_csv('asphaltDB.csv', index=False)