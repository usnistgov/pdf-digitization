# -*- coding: utf-8 -*-
"""gypsum_uuids_scriptfor_DB

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N9CW-vYTd7omZfFn1EUpEg59DSJUg2sz
"""

import os
import json
import pandas as pd
import csv
import requests
import time
from google.colab import userdata

# Extract 'open_xpd_uuid' from all JSON files in a directory
def extract_open_xpd_uuids(json_dir):
  uuids = []
  total_files = 0

  json_files = [f for f in os.listdir(json_dir) if f.endswith('.json')]

  for filename in json_files:
    filepath = os.path.join(json_dir, filename)
    with open(filepath, 'r') as f:
      total_files += 1
      try:
        json_data = json.load(f)
        for obj in json_data:
          uuid = obj.get('open_xpd_uuid')
          if uuid:
            uuids.append(uuid)
      except Exception as e:
        print(f"Error reading {filename}: {e}")

  return uuids, total_files

# Save UUID list to a CSV file
def save_uuids_to_csv(uuids, output_path):
  df = pd.DataFrame(uuids, columns=['open_uuid'])
  df.to_csv(output_path, index=False)
  print(f"Saved {len(df)} UUIDs to {output_path}")

if __name__ == "__main__":
  json_dir = '/content/drive/MyDrive/openEPDs/Category&pcr'
  output_csv = 'gypsum_open_xpd_uuid.csv'
  uuids, file_count = extract_open_xpd_uuids(json_dir)
  save_uuids_to_csv(uuids, output_csv)

# Define the API endpoint
api_endpoint = "https://openepd.buildingtransparency.org/api/epds/{id}"
headers = {
    "Authorization": "Bearer " + userdata.get('BUILDING_TRANSPARENCY_API_KEY')
}

csv_file = '/content/gypsum_open_xpd_uuid.csv'

# Open the CSV file and read the IDs
with open(csv_file, 'r') as csv_file:
    reader = csv.reader(csv_file)
    next(reader)  # Skip the header row
    ids = [row[0] for row in reader]

# Create a list to store the responses
responses = []
count = 0

# Helper function to flatten nested JSON
def flatten_json(data, parent_key=''):
  items = []
  for key, value in data.items():
    new_key = f"{parent_key}/{key}" if parent_key else key
    if isinstance(value, dict):
      items.extend(flatten_json(value, new_key).items())
    else:
      items.append((new_key, value))
  return dict(items)

# Loop through the IDs and make API calls
for id in ids:
  print(count+1)
  time.sleep(3) # throttle api calls to avoid 429 error

  url = api_endpoint.format(id=id)
  response = requests.get(url, headers=headers)

  if response.status_code == 200:
    data = response.json()
    row = {"id": id}
    row.update(flatten_json(data))
    responses.append(row)
  else:
    print(response.status_code, 'error')
    responses.append({"id": id})
  count = count + 1

# Get the unique keys from the responses
all_keys = []
for row in responses:
  for key in row.keys():
    if key not in all_keys:
      all_keys.append(key)
# Optionally ensure 'id' is first
if 'id' in all_keys:
  all_keys.remove('id')
  all_keys.insert(0, 'id')
fieldnames = all_keys

# Save the responses to a CSV file
with open("flat_gypsum_epd_responses.csv", 'w', newline='') as csv_file:
  fieldnames = ['id'] + [key for key in all_keys if key != 'id']
  writer = csv.DictWriter(csv_file, fieldnames=fieldnames)

  writer.writeheader()
  for row in responses:
    writer.writerow(row)